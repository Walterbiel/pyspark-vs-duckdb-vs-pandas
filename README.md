# ğŸš€ AnÃ¡lise de MilhÃµes de Linhas com PySpark, DuckDB e Pandas

Este projeto demonstra como lidar com grandes volumes de dados â€” 50 milhÃµes de linhas â€” utilizando trÃªs tecnologias distintas: **PySpark**, **DuckDB** e **Pandas**.

O objetivo Ã© mostrar os pontos fortes de cada abordagem para leitura, transformaÃ§Ã£o e anÃ¡lise de dados volumosos, comparando suas **performances em um mesmo cenÃ¡rio**.

---

## ğŸ“¦ Estrutura dos Dados

Os dados foram gerados de forma sintÃ©tica com `pandas` e `numpy`, simulando uma base com 50 milhÃµes de registros, contendo as seguintes colunas:

| Coluna     | Tipo     | DescriÃ§Ã£o                                |
|------------|----------|------------------------------------------|
| `id`       | int      | Identificador Ãºnico sequencial           |
| `categoria`| str      | Categoria aleatÃ³ria (`A`, `B`, `C`, `D`) |
| `valor`    | float    | Valor monetÃ¡rio aleatÃ³rio                |
| `data`     | datetime | Datas aleatÃ³rias entre 2023 e 2025       |
| `flag`     | bool     | Valor booleano aleatÃ³rio                 |

---

## ğŸ“Š Consultas Realizadas

1. **Filtro de intervalo de datas + mÃ©dia por categoria**
   - Selecionar registros entre `2023-06-01` e `2024-06-01`
   - Agrupar por categoria e calcular mÃ©dia

2. **Contagem com mÃºltiplas condiÃ§Ãµes**
   - `valor > 500`
   - `flag == True`
   - `categoria != 'C'`

3. **Agrupamento por ano e categoria**
   - Extrair o ano da data
   - Agrupar por ano e categoria
   - Calcular mÃ©dia e soma

4. **Janela de tempo (rolling)**
   - Para cada categoria, calcular a mÃ©dia mÃ³vel de 7 dias

5. **Top N com ordenaÃ§Ã£o**
   - Listar os 10 maiores valores para cada categoria

> âš ï¸ Mais 4 etapas serÃ£o adicionadas em breve nesta seÃ§Ã£o.

---

## ğŸ“ˆ Resultados

### ğŸ”¥ PySpark

| Etapa                          | Tempo (segundos)      |
|-------------------------------|------------------------|
| Leitura e preparaÃ§Ã£o          | 5.10                   |
| Filtro + mÃ©dia                | 1.86                   |
| Contagem com condiÃ§Ãµes        | 0.74                   |
| Agrupamento por ano + categoria | 1.01                 |
| MÃ©dia mÃ³vel                   | 0.38                   |
| Top 10 por categoria          | 1.24                   |
| **Tempo total**               | **10.33**              |

---

### ğŸ¼ Pandas

| Etapa                          | Tempo (segundos)      |
|-------------------------------|------------------------|
| Leitura e preparaÃ§Ã£o          | 0.40                   |
| Filtro + mÃ©dia                | 0.06                   |
| Contagem com condiÃ§Ãµes        | 0.07                   |
| Agrupamento por ano + categoria | 0.12                 |
| MÃ©dia mÃ³vel                   | 0.44                   |
| Top 10 por categoria          | 0.39                   |
| **Tempo total**               | **1.48**               |

---

### ğŸ¦† DuckDB

| Etapa                          | Tempo (segundos)      |
|-------------------------------|------------------------|
| Filtro + mÃ©dia                | 0.18                   |
| Contagem com condiÃ§Ãµes        | 0.12                   |
| Agrupamento por ano + categoria | 0.13                 |
| MÃ©dia mÃ³vel                   | 0.53                   |
| Top 10 por categoria          | 0.37                   |
| **Tempo total**               | **1.33**               |

---

## âš–ï¸ Comparativo entre Tecnologias

| Tecnologia | Pontos Fortes                                      | Quando Usar                                              |
|------------|----------------------------------------------------|-----------------------------------------------------------|
| **PySpark** | EscalÃ¡vel, ideal para clusters e Big Data          | Quando os dados ultrapassam o que cabe em memÃ³ria         |
| **DuckDB**  | SQL embutido, super rÃ¡pido, roda localmente        | Quando vocÃª precisa de agilidade com milhÃµes de registros |
| **Pandas**  | FlexÃ­vel, Ã³timo para visualizaÃ§Ãµes e ajustes finais | Ideal para etapas finais ou amostras menores              |

---

## ğŸ“Œ ConsideraÃ§Ãµes Finais

Este projeto mostra que Ã© possÃ­vel lidar com dezenas de milhÃµes de registros **mesmo em ambiente local**, desde que se escolha a ferramenta adequada para o contexto.

Cada tecnologia tem seu papel, e **combinÃ¡-las de forma estratÃ©gica** pode trazer o melhor de cada uma: escalabilidade, velocidade e flexibilidade.

---

## âœ… PrÃ³ximos Passos

- [ ] Adicionar mais 4 etapas de anÃ¡lise
- [ ] Incluir benchmark com paralelismo no PySpark
- [ ] Publicar resultados detalhados no Medium/LinkedIn
- [ ] Criar versÃ£o com dados reais de domÃ­nio pÃºblico

---

> Projeto criado por **Walter Gonzaga - WGG Digital Solutions**  
> ğŸ“¬ [LinkedIn](https://linkedin.com/in/walter-gonzaga) | ğŸŒ [wggsolutions.com](https://wggsolutions.com)

