# üöÄ An√°lise de 50 Milh√µes de Linhas com PySpark, DuckDB e Pandas

Este projeto demonstra como lidar com grandes volumes de dados (50 milh√µes de linhas) utilizando tr√™s tecnologias diferentes: **PySpark**, **DuckDB** e **Pandas**.

A ideia √© mostrar os pontos fortes de cada abordagem para leitura, transforma√ß√£o e an√°lise de dados volumosos ‚Äî e comparar suas performances em um cen√°rio comum.

---

## üì¶ Estrutura dos Dados

Os dados foram gerados de forma sint√©tica utilizando `pandas` e `numpy`, simulando uma base com 50 milh√µes de registros e as seguintes colunas:

| Coluna     | Tipo      | Descri√ß√£o                            |
|------------|-----------|----------------------------------------|
| `id`       | int       | Identificador √∫nico sequencial        |
| `categoria`| str       | Categoria aleat√≥ria (A, B, C, D)      |
| `valor`    | float     | Valor monet√°rio aleat√≥rio             |
| `data`     | datetime  | Datas aleat√≥rias entre 2023 e 2025    |
| `flag`     | bool      | Valor booleano aleat√≥rio              |


üìä Consultas Avan√ßadas
1. üìÖ Filtro de intervalo de datas + m√©dia por categoria
Selecionar registros entre 2023-06-01 e 2024-06-01

Agrupar por categoria e calcular m√©dia

2. üßÆ Contagem com m√∫ltiplas condi√ß√µes
Contar quantos registros t√™m:

valor > 500

flag == True

categoria != 'C'

3. üîó Agrupamento por ano e categoria
Extrair ano da data

Agrupar por ano e categoria

Calcular m√©dia e soma

4. üßµ Janela de tempo (rolling)
Para cada categoria, calcular a m√©dia m√≥vel de 7 dias

5. üß† Top N com ordena√ß√£o
Listar os 10 maiores valores para cada categoria

---
## RESULTADOS

### Spark
Base com 1 milh√£o de linhas:
etapa                          |tempo_segundos    |
+-------------------------------+------------------+
|Leitura e prepara√ß√£o           |5.102538347244263 |
|Filtro + m√©dia                 |1.8560850620269775|
|Contagem com condi√ß√µes         |0.7441680431365967|
|Agrupamento por ano + categoria|1.0112082958221436|
|M√©dia m√≥vel                    |0.379549503326416 |
|Top 10 por categoria           |1.2397093772888184|
|Tempo total                    |10.33382534980774 |


### Pandas
                             etapa  tempo_segundos
0             Leitura e prepara√ß√£o        0.397176
1                   Filtro + m√©dia        0.062554
2           Contagem com condi√ß√µes        0.073110
3  Agrupamento por ano + categoria        0.118819
4                      M√©dia m√≥vel        0.442486
5             Top 10 por categoria        0.387176
6                      Tempo total        1.481698

### duckdb
                        etapa  tempo_segundos
0                   Filtro + m√©dia        0.182756
1           Contagem com condi√ß√µes        0.116865
2  Agrupamento por ano + categoria        0.129946
3                      M√©dia m√≥vel        0.530592
4             Top 10 por categoria        0.365201
5                      Tempo total        1.330935


üìä Compara√ß√£o entre Tecnologias
Tecnologia	Pontos Fortes	Quando Usar
PySpark	Escal√°vel, ideal para clusters e Big Data	Quando os dados ultrapassam o que cabe em mem√≥ria
DuckDB	SQL embutido, super r√°pido, roda localmente	Quando voc√™ precisa agilidade com milh√µes de registros
Pandas	Flex√≠vel, √≥timo para visualiza√ß√µes e ajustes	Ideal para etapas finais ou amostras pequenas

üìå Considera√ß√µes
Este projeto mostra que √© poss√≠vel lidar com milh√µes de registros mesmo em ambiente local, escolhendo bem as ferramentas. Cada tecnologia tem seu papel e combin√°-las pode ser a melhor estrat√©gia!# pyspark-vs-duckdb-vs-pandas
