# ğŸš€ AnÃ¡lise de 50 MilhÃµes de Linhas com PySpark, DuckDB e Pandas

Este projeto demonstra como lidar com grandes volumes de dados (50 milhÃµes de linhas) utilizando trÃªs tecnologias diferentes: **PySpark**, **DuckDB** e **Pandas**.

A ideia Ã© mostrar os pontos fortes de cada abordagem para leitura, transformaÃ§Ã£o e anÃ¡lise de dados volumosos â€” e comparar suas performances em um cenÃ¡rio comum.

---

## ğŸ“¦ Estrutura dos Dados

Os dados foram gerados de forma sintÃ©tica utilizando `pandas` e `numpy`, simulando uma base com 50 milhÃµes de registros e as seguintes colunas:

| Coluna     | Tipo      | DescriÃ§Ã£o                            |
|------------|-----------|----------------------------------------|
| `id`       | int       | Identificador Ãºnico sequencial        |
| `categoria`| str       | Categoria aleatÃ³ria (A, B, C, D)      |
| `valor`    | float     | Valor monetÃ¡rio aleatÃ³rio             |
| `data`     | datetime  | Datas aleatÃ³rias entre 2023 e 2025    |
| `flag`     | bool      | Valor booleano aleatÃ³rio              |


ğŸ“Š Consultas AvanÃ§adas
1. ğŸ“… Filtro de intervalo de datas + mÃ©dia por categoria
Selecionar registros entre 2023-06-01 e 2024-06-01

Agrupar por categoria e calcular mÃ©dia

2. ğŸ§® Contagem com mÃºltiplas condiÃ§Ãµes
Contar quantos registros tÃªm:

valor > 500

flag == True

categoria != 'C'

3. ğŸ”— Agrupamento por ano e categoria
Extrair ano da data

Agrupar por ano e categoria

Calcular mÃ©dia e soma

4. ğŸ§µ Janela de tempo (rolling)
Para cada categoria, calcular a mÃ©dia mÃ³vel de 7 dias

5. ğŸ§  Top N com ordenaÃ§Ã£o
Listar os 10 maiores valores para cada categoria

---

## ğŸ§ª GeraÃ§Ã£o dos Dados

```python
import pandas as pd
import numpy as np

n = 50_000_000

np.random.seed(42)

df = pd.DataFrame({
    'id': np.arange(n),
    'categoria': np.random.choice(['A', 'B', 'C', 'D'], size=n),
    'valor': np.random.uniform(0, 1000, size=n).round(2),
    'data': pd.to_datetime('2023-01-01') + pd.to_timedelta(np.random.randint(0, 730, size=n), unit='D'),
    'flag': np.random.choice([True, False], size=n)
})

df.to_parquet('dados_50_milhoes.parquet', index=False)

```


ğŸ”¥ AnÃ¡lise com PySpark
python
Copy
Edit
from pyspark.sql import SparkSession
from pyspark.sql.functions import avg

spark = SparkSession.builder.appName("Analise50Milhoes").getOrCreate()
df_spark = spark.read.parquet("dados_50_milhoes.parquet")

df_resultado = df_spark.groupBy("categoria").agg(avg("valor").alias("media_valor"))
df_resultado.show()

ğŸ“Š ComparaÃ§Ã£o entre Tecnologias
Tecnologia	Pontos Fortes	Quando Usar
PySpark	EscalÃ¡vel, ideal para clusters e Big Data	Quando os dados ultrapassam o que cabe em memÃ³ria
DuckDB	SQL embutido, super rÃ¡pido, roda localmente	Quando vocÃª precisa agilidade com milhÃµes de registros
Pandas	FlexÃ­vel, Ã³timo para visualizaÃ§Ãµes e ajustes	Ideal para etapas finais ou amostras pequenas

ğŸ“Œ ConsideraÃ§Ãµes
Este projeto mostra que Ã© possÃ­vel lidar com milhÃµes de registros mesmo em ambiente local, escolhendo bem as ferramentas. Cada tecnologia tem seu papel e combinÃ¡-las pode ser a melhor estratÃ©gia!# pyspark-vs-duckdb-vs-pandas
